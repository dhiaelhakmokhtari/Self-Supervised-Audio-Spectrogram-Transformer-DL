{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Drafts and Various Versions"
      ],
      "metadata": {
        "id": "Wh2-k5pPdIi_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf0k-C1ofyEu"
      },
      "source": [
        "##### imports and configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUtSbs8xf6Fm",
        "outputId": "28e18fd0-1ac7-4514-ce86-dfdf25ec23ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#os.chdir('/MyDrive/SSAST')\n",
        "\n",
        "# Check if GPU is available (likely CPU if local, which is fine for debugging)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elPV__umgg4O"
      },
      "source": [
        "##### Version 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy2Id9jogjzA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "from pydub.exceptions import CouldntDecodeError\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ðŸ”¹ FFmpeg setup\n",
        "#ffmpeg_dir = r\"C:\\ffmpeg\\bin\"\n",
        "#os.environ[\"PATH\"] += os.pathsep + ffmpeg_dir\n",
        "#AudioSegment.converter = os.path.join(ffmpeg_dir, \"ffmpeg.exe\")\n",
        "#AudioSegment.ffprobe   = os.path.join(ffmpeg_dir, \"ffprobe.exe\")\n",
        "\n",
        "# ðŸ”¹ Dataset paths\n",
        "input_dataset = \"./data/genres_original\"\n",
        "output_dataset = \"./data/gtzan_10s_1\"\n",
        "segment_length = 10 * 1000  # 10 seconds in milliseconds\n",
        "\n",
        "os.makedirs(output_dataset, exist_ok=True)\n",
        "genres = [g for g in os.listdir(input_dataset) if os.path.isdir(os.path.join(input_dataset, g))]\n",
        "\n",
        "# Dictionary to store counts per genre\n",
        "genre_counts = {}\n",
        "\n",
        "for genre in genres:\n",
        "    genre_path = os.path.join(input_dataset, genre)\n",
        "    out_dir = os.path.join(output_dataset, genre)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\nProcessing genre: {genre}\")\n",
        "    count = 0\n",
        "    for file in tqdm(os.listdir(genre_path)):\n",
        "        if not file.endswith(\".wav\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(genre_path, file)\n",
        "        try:\n",
        "            audio = AudioSegment.from_wav(file_path)\n",
        "        except CouldntDecodeError:\n",
        "            print(f\"âš  Could not decode {file_path}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        duration = len(audio)\n",
        "        for i in range(0, duration, segment_length):\n",
        "            chunk = audio[i:i + segment_length]\n",
        "            if len(chunk) < segment_length:\n",
        "                continue  # skip short last chunk\n",
        "\n",
        "            filename = f\"{file.replace('.wav','')}_{i//segment_length}.wav\"\n",
        "            chunk.export(os.path.join(out_dir, filename), format=\"wav\")\n",
        "            count += 1\n",
        "\n",
        "    genre_counts[genre] = count\n",
        "\n",
        "# Print summary of counts\n",
        "print(\"\\nâœ… Done! Summary of 10s segments per genre:\")\n",
        "for genre, count in genre_counts.items():\n",
        "    print(f\"{genre}: {count} segments\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwAPWjTX40RC",
        "outputId": "1c3461e7-f862-431c-9019-a1bdbce6e0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset Ready! Found 2988 samples.\n",
            "Classes: ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Define Dataset Class\n",
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "class GTZANDataset(Dataset):\n",
        "    def __init__(self, data_path, target_length=1024):\n",
        "        # Find all .wav files in subfolders\n",
        "        self.files = sorted(glob.glob(os.path.join(data_path, \"*\", \"*.wav\")))\n",
        "\n",
        "        # Create label mapping (rock -> 0, jazz -> 1, etc.)\n",
        "        self.classes = sorted(list(set([f.split(os.sep)[-2] for f in self.files])))\n",
        "        self.cls_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "\n",
        "        # SSAST Normalization Stats\n",
        "        self.norm_mean = -4.2677393\n",
        "        self.norm_std = 4.5689974\n",
        "        self.target_length = target_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav_path = self.files[idx]\n",
        "        label = self.cls_to_idx[wav_path.split(os.sep)[-2]]\n",
        "\n",
        "        # 1. Robust Load using SoundFile\n",
        "        # This avoids the \"No backend\" error you saw earlier\n",
        "        audio_np, sr = sf.read(wav_path)\n",
        "        waveform = torch.from_numpy(audio_np).float()\n",
        "\n",
        "        # Ensure shape is [1, time]\n",
        "        if waveform.dim() == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        # 2. Resample to 16kHz (Required by SSAST)\n",
        "        if sr != 16000:\n",
        "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
        "\n",
        "        # 3. Create Spectrogram (Kaldi Fbank matches paper)\n",
        "        fbank = torchaudio.compliance.kaldi.fbank(\n",
        "            waveform,\n",
        "            htk_compat=True,\n",
        "            sample_frequency=16000,\n",
        "            use_energy=False,\n",
        "            window_type='hanning',\n",
        "            num_mel_bins=128,\n",
        "            dither=0.0,\n",
        "            frame_shift=10\n",
        "        )\n",
        "\n",
        "        # 4. Force Exact Length (Padding/Cropping)\n",
        "        n_frames = fbank.shape[0]\n",
        "        p = self.target_length - n_frames\n",
        "        if p > 0:\n",
        "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
        "            fbank = m(fbank)\n",
        "        elif p < 0:\n",
        "            fbank = fbank[:self.target_length, :]\n",
        "\n",
        "        # 5. Normalize\n",
        "        fbank = (fbank - self.norm_mean) / (self.norm_std * 2)\n",
        "\n",
        "        return fbank, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# Verify it works\n",
        "ds = GTZANDataset(\"./data/gtzan_10s\")\n",
        "print(f\"âœ… Dataset Ready! Found {len(ds)} samples.\")\n",
        "print(f\"Classes: {ds.classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfZMk5Sv8pbR",
        "outputId": "7bb94508-5151-41d2-9ebe-238351a611d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Size: 2390\n",
            "Test Size: 598\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Data Loaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 80% Train / 20% Test\n",
        "train_size = int(0.8 * len(ds))\n",
        "test_size = len(ds) - train_size\n",
        "train_ds, test_ds = random_split(ds, [train_size, test_size])\n",
        "\n",
        "print(f\"Train Size: {len(train_ds)}\")\n",
        "print(f\"Test Size: {len(test_ds)}\")\n",
        "\n",
        "# Batch size 16 is safe for Colab's T4 GPU\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYAWQnXL8tQp",
        "outputId": "ba80faa6-18a6-42ee-c012-1f05ac5d3e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ResNet-18 Baseline Loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Baseline CNN Architecture\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BaselineCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(BaselineCNN, self).__init__()\n",
        "\n",
        "        # 4 Conv Blocks\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Classifier\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(256 * 64 * 8, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add channel dimension [Batch, 1, Time, Freq]\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eon9QQDT8vzc",
        "outputId": "292f539d-27db-45f5-b176-f979c1ce771b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Universal Training Engine\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def train_engine(model, epochs, lr):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'acc': [], 'loss': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(train_loader):.4f} | Test Acc: {acc:.2f}%\")\n",
        "        history['acc'].append(acc)\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "2b65day08yy0",
        "outputId": "9c1e182f-5ceb-4c01-f3b5-840ce3610f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Training Baseline CNN...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "an integer is required",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3689045922.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Standard LR for scratch training is 0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcnn_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3592590078.py\u001b[0m in \u001b[0;36mtrain_engine\u001b[0;34m(model, epochs, lr)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'TypeError'>, TypeError('an integer is required'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2104\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2105\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_pdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m                         \u001b[0;31m# drop into debugger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36m_showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;34m'traceback'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;34m'ename'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpy3compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;34m'evalue'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpy3compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     }\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipython_genutils/py3compat.py\u001b[0m in \u001b[0;36msafe_unicode\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/soundfile.py\u001b[0m in \u001b[0;36merror_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0;34m\"\"\"Raw libsndfile error message.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m             \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: an integer is required"
          ]
        }
      ],
      "source": [
        "# Cell 8: Run Baseline Training\n",
        "cnn_model = BaselineCNN(num_classes=10)\n",
        "print(\"ðŸš€ Training Baseline CNN...\")\n",
        "# Standard LR for scratch training is 0.001\n",
        "cnn_history = train_engine(cnn_model, epochs=20, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKpXoe2WGOad"
      },
      "source": [
        "##### Version 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6tMTBp5GVEs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Paths\n",
        "DATASET_PATH = \"/content/drive/MyDrive/SSAST/data/gtzan_10s\"  # sliced 10s dataset\n",
        "TARGET_SR = 16000                  # SSAST requirement\n",
        "NUM_MEL_BINS = 128\n",
        "FRAME_LENGTH_MS = 25\n",
        "FRAME_SHIFT_MS = 10\n",
        "NUM_FRAMES = 1024                  # model input length\n",
        "\n",
        "# AudioSet statistics for normalization\n",
        "AUDIOSET_MEAN = -4.2677393\n",
        "AUDIOSET_STD = 4.5689974 * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfjS3KmLGe4u"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class GTZANDataset(Dataset):\n",
        "    def __init__(self, root_dir, apply_augment=True):\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "        self.class2idx = {}\n",
        "        self.apply_augment = apply_augment  # toggle ON/OFF\n",
        "\n",
        "        classes = sorted(os.listdir(root_dir))\n",
        "        for idx, cls in enumerate(classes):\n",
        "            self.class2idx[cls] = idx\n",
        "            cls_dir = os.path.join(root_dir, cls)\n",
        "\n",
        "            for file in os.listdir(cls_dir):\n",
        "                if file.endswith(\".wav\"):\n",
        "                    self.file_paths.append(os.path.join(cls_dir, file))\n",
        "                    self.labels.append(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path  = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # ===========================\n",
        "        #  SAFE LOAD (Bad File Skip)\n",
        "        # ===========================\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(path)\n",
        "        except Exception:\n",
        "            print(f\"âš  Bad file skipped: {path}\")\n",
        "            return torch.zeros(NUM_FRAMES, NUM_MEL_BINS), label   # keep batch intact\n",
        "\n",
        "        # ===========================\n",
        "        #   RESAMPLE â†’ 16 kHz fixed\n",
        "        # ===========================\n",
        "        if sr != TARGET_SR:\n",
        "            waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SR)(waveform)\n",
        "\n",
        "        # ===========================\n",
        "        #     KALDI FBank Extract\n",
        "        # ===========================\n",
        "        fbank = torchaudio.compliance.kaldi.fbank(\n",
        "            waveform,\n",
        "            num_mel_bins=NUM_MEL_BINS,\n",
        "            frame_length=FRAME_LENGTH_MS,\n",
        "            frame_shift=FRAME_SHIFT_MS\n",
        "        )\n",
        "\n",
        "        # ===========================\n",
        "        #      â†ª SpecAugment â†©\n",
        "        # (Before Normalization)\n",
        "        # ===========================\n",
        "        if self.apply_augment and random.random() < 0.5:\n",
        "            fbank = torchaudio.transforms.FrequencyMasking(freq_mask_param=12)(fbank)\n",
        "            fbank = torchaudio.transforms.TimeMasking(time_mask_param=12)(fbank)\n",
        "\n",
        "        # ===========================\n",
        "        #   Shape must be 1024 Ã— 128\n",
        "        # ===========================\n",
        "        T = fbank.shape[0]\n",
        "        if T < NUM_FRAMES:\n",
        "            fbank = torch.cat([fbank, torch.zeros(NUM_FRAMES - T, NUM_MEL_BINS)], dim=0)\n",
        "        else:\n",
        "            fbank = fbank[:NUM_FRAMES]\n",
        "\n",
        "        # ===========================\n",
        "        #  Normalize (AudioSet spec)\n",
        "        # ===========================\n",
        "        fbank = (fbank - AUDIOSET_MEAN) / AUDIOSET_STD\n",
        "\n",
        "        return fbank, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sI1brw4Gr2c",
        "outputId": "8c4f3d13-68e8-4975-9c1e-89a0b9b05f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 2390, Validation samples: 598\n"
          ]
        }
      ],
      "source": [
        "dataset = GTZANDataset(DATASET_PATH)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Train samples: {len(train_set)}, Validation samples: {len(val_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz-AKJ6KGuLI"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "        )\n",
        "\n",
        "        # FIXED FC SHAPE\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 128 * 16, 256),  # <â€” corrected\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.conv_layers(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEdlMpMTGwf7"
      },
      "outputs": [],
      "source": [
        "model = SimpleCNN(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa8705iNGzIu",
        "outputId": "c77e7260-e79e-4469-e166-03ef7ce811f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 | Loss: 2.1724 | LR: 0.001000\n",
            "Epoch 2/30 | Loss: 2.1626 | LR: 0.001000\n",
            "Epoch 3/30 | Loss: 2.1147 | LR: 0.001000\n",
            "Epoch 4/30 | Loss: 2.1155 | LR: 0.001000\n",
            "Epoch 5/30 | Loss: 2.0896 | LR: 0.001000\n",
            "Epoch 6/30 | Loss: 2.0820 | LR: 0.001000\n",
            "Epoch 7/30 | Loss: 2.0451 | LR: 0.001000\n",
            "Epoch 8/30 | Loss: 2.0618 | LR: 0.001000\n",
            "Epoch 9/30 | Loss: 2.0532 | LR: 0.001000\n",
            "Epoch 10/30 | Loss: 2.0381 | LR: 0.000500\n",
            "Epoch 11/30 | Loss: 1.9818 | LR: 0.000500\n",
            "Epoch 12/30 | Loss: 1.9890 | LR: 0.000500\n",
            "Epoch 13/30 | Loss: 1.9745 | LR: 0.000500\n",
            "Epoch 14/30 | Loss: 1.9411 | LR: 0.000500\n",
            "Epoch 15/30 | Loss: 1.9573 | LR: 0.000500\n",
            "Epoch 16/30 | Loss: 1.9112 | LR: 0.000500\n",
            "Epoch 17/30 | Loss: 1.9204 | LR: 0.000500\n",
            "Epoch 18/30 | Loss: 1.8965 | LR: 0.000500\n",
            "Epoch 19/30 | Loss: 1.8808 | LR: 0.000500\n",
            "Epoch 20/30 | Loss: 1.8627 | LR: 0.000250\n",
            "Epoch 21/30 | Loss: 1.8264 | LR: 0.000250\n",
            "Epoch 22/30 | Loss: 1.8120 | LR: 0.000250\n",
            "Epoch 23/30 | Loss: 1.8179 | LR: 0.000250\n",
            "Epoch 24/30 | Loss: 1.8165 | LR: 0.000250\n",
            "Epoch 25/30 | Loss: 1.7977 | LR: 0.000250\n",
            "Epoch 26/30 | Loss: 1.7875 | LR: 0.000250\n",
            "Epoch 27/30 | Loss: 1.7842 | LR: 0.000250\n",
            "Epoch 28/30 | Loss: 1.7764 | LR: 0.000250\n",
            "Epoch 29/30 | Loss: 1.7714 | LR: 0.000250\n",
            "Epoch 30/30 | Loss: 1.7643 | LR: 0.000125\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "EPOCHS = 30                           # increased training time\n",
        "scheduler = StepLR(optimizer,\n",
        "                   step_size=10,      # reduce LR every 10 epochs\n",
        "                   gamma=0.5)         # new_lr = old_lr * 0.5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.unsqueeze(1).to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X)\n",
        "        loss = criterion(out, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * X.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # ðŸ”¥ decays LR at scheduled epochs\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p17hMlqOG2EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37fe72b7-969a-42ca-bbfe-c706576b3a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Validation Accuracy: 0.3010\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Cell 7: Validation Accuracy\n",
        "# ========================================\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for X, y in val_loader:\n",
        "        X, y = X.unsqueeze(1).to(device), y.to(device)\n",
        "        outputs = model(X)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "print(f\"Validation Accuracy: {correct/total:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHKsZc1HZsis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559e9eaf-1912-4d00-9766-afa4e16af6ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ResNet-18 Baseline Loaded (approx 11M parameters).\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 1/25 | Loss: 1.5859 | Train Acc: 44.56%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 2/25 | Loss: 1.2755 | Train Acc: 55.48%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 3/25 | Loss: 1.0668 | Train Acc: 63.77%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 4/25 | Loss: 0.9157 | Train Acc: 68.66%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 5/25 | Loss: 0.7983 | Train Acc: 71.63%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 6/25 | Loss: 0.7006 | Train Acc: 75.52%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 7/25 | Loss: 0.6372 | Train Acc: 78.28%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 8/25 | Loss: 0.5740 | Train Acc: 80.67%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 9/25 | Loss: 0.5209 | Train Acc: 82.38%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 10/25 | Loss: 0.4553 | Train Acc: 84.23%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 11/25 | Loss: 0.2887 | Train Acc: 90.67%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 12/25 | Loss: 0.2202 | Train Acc: 92.59%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 13/25 | Loss: 0.2003 | Train Acc: 93.22%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 14/25 | Loss: 0.1922 | Train Acc: 94.06%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 15/25 | Loss: 0.1810 | Train Acc: 94.39%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 16/25 | Loss: 0.1765 | Train Acc: 94.85%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 17/25 | Loss: 0.1552 | Train Acc: 95.31%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 18/25 | Loss: 0.1340 | Train Acc: 96.36%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 19/25 | Loss: 0.1179 | Train Acc: 96.86%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 20/25 | Loss: 0.1064 | Train Acc: 97.24%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 21/25 | Loss: 0.0987 | Train Acc: 97.20%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 22/25 | Loss: 0.0931 | Train Acc: 97.32%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 23/25 | Loss: 0.0950 | Train Acc: 97.62%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 24/25 | Loss: 0.0850 | Train Acc: 97.91%\n",
            "âš  Bad file skipped: /content/drive/MyDrive/SSAST/data/gtzan_10s/reggae/reggae.00027_2.wav\n",
            "Epoch 25/25 | Loss: 0.1029 | Train Acc: 97.45%\n",
            "\n",
            "ðŸ† Final Validation Accuracy: 89.63%\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "def get_resnet_baseline(num_classes=10):\n",
        "    # 1. Load standard ResNet-18\n",
        "    model = models.resnet18(weights=None) # Train from scratch\n",
        "\n",
        "    # 2. Modify Input Layer\n",
        "    # ResNet expects 3-channel RGB. We have 1-channel Spectrograms.\n",
        "    # We change the first layer to accept 1 input channel.\n",
        "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "    # 3. Modify Output Layer\n",
        "    # ResNet outputs 1000 classes (ImageNet). We need 10 (GTZAN).\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize\n",
        "model = get_resnet_baseline(num_classes=10).to(device)\n",
        "\n",
        "# Setup Training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "print(\"âœ… ResNet-18 Baseline Loaded (approx 11M parameters).\")\n",
        "\n",
        "# --- Training Loop ---\n",
        "EPOCHS = 25\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.unsqueeze(1).to(device), y.to(device) # [Batch, 1, Time, Freq]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = outputs.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += preds.eq(y).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    train_acc = 100. * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "# --- Validation Check ---\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for X, y in val_loader:\n",
        "        X, y = X.unsqueeze(1).to(device), y.to(device)\n",
        "        outputs = model(X)\n",
        "        _, preds = outputs.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += preds.eq(y).sum().item()\n",
        "\n",
        "print(f\"\\nðŸ† Final Validation Accuracy: {100. * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Version 3"
      ],
      "metadata": {
        "id": "vteisd-H5SeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GTZAN10sDataset(Dataset):\n",
        "    def __init__(self, csv_path, class_mapping, augment=False):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.class_mapping = class_mapping\n",
        "        self.augment = augment\n",
        "        self.target_sr = 16000\n",
        "        self.target_length = 1024 # 1024 frames corresponds to ~10.24 seconds\n",
        "\n",
        "        # SSAST AudioSet Statistics (Required for Transfer Learning)\n",
        "        self.norm_mean = -4.2677393\n",
        "        self.norm_std = 4.5689974\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        audio_path = row[\"filepath\"]\n",
        "        # Ensure we get the label correctly (handle string vs int)\n",
        "        label_str = row[\"label\"]\n",
        "        label = self.class_mapping[label_str] if isinstance(label_str, str) else label_str\n",
        "\n",
        "        # 1. Load audio\n",
        "        # Note: If you are on Windows and get backend errors, stick to soundfile.read()\n",
        "        waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "        # 2. Resample to 16kHz\n",
        "        if sr != self.target_sr:\n",
        "            waveform = torchaudio.transforms.Resample(sr, self.target_sr)(waveform)\n",
        "\n",
        "        # 3. Mono conversion (if needed)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # 4. Kaldi FBank Extraction (Matches SSAST paper exactly)\n",
        "        # Replaces MelSpectrogram + AmplitudeToDB\n",
        "        fbank = torchaudio.compliance.kaldi.fbank(\n",
        "            waveform,\n",
        "            htk_compat=True,\n",
        "            sample_frequency=16000,\n",
        "            use_energy=False,\n",
        "            window_type='hanning',\n",
        "            num_mel_bins=128,\n",
        "            dither=0.0,\n",
        "            frame_shift=10\n",
        "        )\n",
        "\n",
        "        # 5. Padding / Cropping to 1024 Frames\n",
        "        n_frames = fbank.shape[0]\n",
        "        p = self.target_length - n_frames\n",
        "\n",
        "        if p > 0:\n",
        "            # Pad with zeros if too short\n",
        "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
        "            fbank = m(fbank)\n",
        "        elif p < 0:\n",
        "            # Crop if too long\n",
        "            fbank = fbank[0:self.target_length, :]\n",
        "\n",
        "        # 6. Normalize using AudioSet Stats\n",
        "        # The formula includes * 2 as per the SSAST repository\n",
        "        fbank = (fbank - self.norm_mean) / (self.norm_std * 2)\n",
        "\n",
        "        # Output shape is [1024, 128]\n",
        "        return fbank, torch.tensor(label, dtype=torch.long)"
      ],
      "metadata": {
        "id": "mGL_3hmz5UUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "from pydub.exceptions import CouldntDecodeError\n",
        "from tqdm import tqdm\n",
        "\n",
        "class_mapping = {\n",
        "    \"blues\": 0, \"classical\": 1, \"country\": 2, \"disco\": 3,\n",
        "    \"hiphop\": 4, \"jazz\": 5, \"metal\": 6, \"pop\": 7,\n",
        "    \"reggae\": 8, \"rock\": 9\n",
        "}\n",
        "\n",
        "train_set = GTZAN10sDataset(\"./metadata/train.csv\", class_mapping)\n",
        "print(f\"Number of samples: {len(train_set)}\")\n",
        "\n",
        "\n",
        "fbank, label = train_set[0]\n",
        "print(f\"Spectrogram shape: {fbank.shape}\")   # Should be [NUM_FRAMES, NUM_MEL_BINS]\n",
        "print(f\"Label: {label}\")                     # Should be an int 0â€“9"
      ],
      "metadata": {
        "id": "XyLQ5eI5874i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Working SSAST Implementation"
      ],
      "metadata": {
        "id": "-dHtXftddBXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "version 4"
      ],
      "metadata": {
        "id": "JIEbLaNFFlKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Environment Setup\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Install Audio Libraries (Linux System + Python)\n",
        "!apt-get update -qq && apt-get install -y libsndfile1 ffmpeg > /dev/null\n",
        "!pip install timm==0.4.5 torchaudio soundfile pandas matplotlib > /dev/null\n",
        "\n",
        "print(\" Environment Ready.\")"
      ],
      "metadata": {
        "id": "Xhl6eQR9J_jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39c7334-4d97-4431-9d42-2217ef5d9d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "âœ… Environment Ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Transfer Data for Speed\n",
        "from pathlib import Path\n",
        "\n",
        "# --- CONFIGURATION: EDIT THIS PATH ---\n",
        "# Where did you put your files in Drive?\n",
        "DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/SSAST_Project\"\n",
        "# -------------------------------------\n",
        "\n",
        "def setup_local_environment():\n",
        "    print(f\"Setting up from: {DRIVE_PROJECT_PATH}\")\n",
        "\n",
        "    # 1. Copy Model Code (ast_models.py) to current directory\n",
        "    # This allows us to do 'from ast_models import ASTModel' easily\n",
        "    if not os.path.exists('ast_models.py'):\n",
        "        shutil.copy(f\"{DRIVE_PROJECT_PATH}/ast_models.py\", \".\")\n",
        "        print(\"Model code copied.\")\n",
        "\n",
        "    # 2. Copy Metadata CSVs\n",
        "    if not os.path.exists('metadata'):\n",
        "        shutil.copytree(f\"{DRIVE_PROJECT_PATH}/metadata\", \"metadata\")\n",
        "        print(\"Metadata copied.\")\n",
        "\n",
        "    # 3. Copy Audio Data (The big one)\n",
        "    # Checks if we already have it locally to save time on re-runs\n",
        "    if not os.path.exists('./data/gtzan_10s'):\n",
        "        print(\"Copying audio files from Drive to Local Disk (this is faster for training)...\")\n",
        "        # Ensure destination dir exists\n",
        "        os.makedirs('./data', exist_ok=True)\n",
        "\n",
        "        # Copy the folder\n",
        "        shutil.copytree(f\"{DRIVE_PROJECT_PATH}/data/gtzan_10s\", \"./data/gtzan_10s\")\n",
        "        print(\"Audio data copied successfully!\")\n",
        "    else:\n",
        "        print(\"Audio data already present locally.\")\n",
        "\n",
        "setup_local_environment()"
      ],
      "metadata": {
        "id": "q9cgoDESKA6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2857c31f-2ee9-48c6-af81-65d6475187b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ Setting up from: /content/drive/MyDrive/SSAST_Project\n",
            "âœ… Audio data already present locally.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GTZAN_Metadata_Dataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.target_sr = 16000\n",
        "        self.target_length = 1024 # frames\n",
        "\n",
        "        # Mapping from string labels to int\n",
        "        self.classes = sorted(self.df['label'].unique().tolist())\n",
        "        self.cls_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "\n",
        "        # SSAST Normalization Stats\n",
        "        self.norm_mean = -4.2677393\n",
        "        self.norm_std = 4.5689974\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # FIX PATH: The CSV might have \"data/gtzan_10s/...\" or Windows paths.\n",
        "        # We enforce the local Linux path we created in Cell 2.\n",
        "        filename = os.path.basename(row['filepath']) # e.g. \"blues.00000_0.wav\"\n",
        "        label_str = row['label']\n",
        "        genre_folder = label_str # Usually the folder name is the genre name\n",
        "\n",
        "        # Construct the safe local path\n",
        "        audio_path = f\"./data/gtzan_10s/{genre_folder}/{filename}\"\n",
        "\n",
        "        label = self.cls_to_idx[label_str]\n",
        "\n",
        "        # 1. Load\n",
        "        audio_np, sr = sf.read(audio_path)\n",
        "        waveform = torch.from_numpy(audio_np).float()\n",
        "        if waveform.dim() == 1: waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        # 2. Resample\n",
        "        if sr != 16000:\n",
        "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
        "\n",
        "        # 3. Spectrogram\n",
        "        fbank = torchaudio.compliance.kaldi.fbank(\n",
        "            waveform, htk_compat=True, sample_frequency=16000, use_energy=False,\n",
        "            window_type='hanning', num_mel_bins=128, dither=0.0, frame_shift=10\n",
        "        )\n",
        "\n",
        "        # 4. Pad/Crop\n",
        "        n_frames = fbank.shape[0]\n",
        "        p = self.target_length - n_frames\n",
        "        if p > 0:\n",
        "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
        "            fbank = m(fbank)\n",
        "        elif p < 0:\n",
        "            fbank = fbank[:self.target_length, :]\n",
        "\n",
        "        # 5. Normalize\n",
        "        fbank = (fbank - self.norm_mean) / (self.norm_std * 2)\n",
        "\n",
        "        return fbank, label\n",
        "\n",
        "# Load Data\n",
        "train_ds = GTZAN_Metadata_Dataset(\"metadata/train.csv\")\n",
        "val_ds   = GTZAN_Metadata_Dataset(\"metadata/val.csv\")\n",
        "test_ds  = GTZAN_Metadata_Dataset(\"metadata/test.csv\")\n",
        "\n",
        "# Create Loaders\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Data Ready: {len(train_ds)} Train, {len(val_ds)} Val.\")"
      ],
      "metadata": {
        "id": "Pnow1HXbKDCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0df460-9a8e-475f-d7b1-09733e907770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data Ready: 2389 Train, 295 Val.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Data Integrity Check\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def check_data_health(csv_path):\n",
        "    print(f\"ðŸ©º Checking health of dataset: {csv_path}\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find {csv_path}\")\n",
        "        return\n",
        "\n",
        "    bad_files = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        file_path = row['filepath']\n",
        "\n",
        "        # 1. Check if file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            bad_files.append((file_path, \"File Not Found\"))\n",
        "            continue\n",
        "\n",
        "        # 2. Check if file is empty\n",
        "        if os.path.getsize(file_path) < 100: # WAV header is ~44 bytes\n",
        "            bad_files.append((file_path, \"File Empty (<100 bytes)\"))\n",
        "            continue\n",
        "\n",
        "        # 3. Try to read audio\n",
        "        try:\n",
        "            sf.read(file_path)\n",
        "        except Exception as e:\n",
        "            # We catch the generic exception because soundfile might crash printing the specific one\n",
        "            bad_files.append((file_path, \"Corrupt Audio Header/Data\"))\n",
        "\n",
        "    if len(bad_files) == 0:\n",
        "        print(\"\\nDataset is HEALTHY! No corrupt files found.\")\n",
        "    else:\n",
        "        print(f\"\\n FOUND {len(bad_files)} BAD FILES!\")\n",
        "        print(\"Here are the first 5 culprits:\")\n",
        "        for path, reason in bad_files[:5]:\n",
        "            print(f\"  - {path} : {reason}\")\n",
        "\n",
        "        print(\"\\n RECOMMENDATION: Delete these files from your 'data' folder and remove them from your CSV.\")\n",
        "\n",
        "# Run check on Train and Val\n",
        "check_data_health(\"metadata/train.csv\")\n",
        "check_data_health(\"metadata/val.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPhJk-ZTO8kf",
        "outputId": "be505681-0cd5-4667-80e7-0b27980d9ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ©º Checking health of dataset: metadata/train.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2389/2389 [00:02<00:00, 899.05it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Dataset is HEALTHY! No corrupt files found.\n",
            "ðŸ©º Checking health of dataset: metadata/val.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [00:00<00:00, 908.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Dataset is HEALTHY! No corrupt files found.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Fix Corrupt CSV Entry\n",
        "import pandas as pd\n",
        "\n",
        "csv_path = \"metadata/train.csv\"\n",
        "bad_file = \"./data/gtzan_10s/reggae/reggae.00027_2.wav\"\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv(csv_path)\n",
        "initial_len = len(df)\n",
        "\n",
        "# Filter out the bad file\n",
        "df = df[df['filepath'] != bad_file]\n",
        "\n",
        "# Save back\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"   Fixed {csv_path}\")\n",
        "print(f\"   Removed: {bad_file}\")\n",
        "print(f\"   Rows: {initial_len} -> {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "qGNf3AK4T9ZS",
        "outputId": "00fdeb33-9b6c-4b97-a033-4313d41cf57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'metadata/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2567515972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0minitial_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'metadata/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: SSAST Training (Memory Optimized)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from ast_models import ASTModel\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "WEIGHTS_PATH = \"/content/drive/MyDrive/SSAST_Project/SSAST-Base-Patch-400.pth\"\n",
        "# Reduced batch size to fit in memory\n",
        "BATCH_SIZE = 4\n",
        "# Accumulate gradients to simulate batch size of 32 (8 * 4 = 32)\n",
        "ACCUM_STEPS = 8\n",
        "# ---------------------\n",
        "\n",
        "# 1. Setup Device & Loaders with new batch size\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Re-create loaders with smaller batch size\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# 2. Initialize Model\n",
        "print(f\"Initializing SSAST...\")\n",
        "ssast_model = ASTModel(\n",
        "    label_dim=527, input_fdim=128, input_tdim=1024,\n",
        "    model_size='base', pretrain_stage=False,\n",
        "    load_pretrained_mdl_path=WEIGHTS_PATH,\n",
        "    fshape=16, tshape=16, fstride=10, tstride=10\n",
        ")\n",
        "\n",
        "# 3. Modify Head\n",
        "if hasattr(ssast_model, 'mlp_head'):\n",
        "    input_dim = ssast_model.mlp_head[1].in_features\n",
        "    ssast_model.mlp_head[1] = nn.Linear(input_dim, 10)\n",
        "    print(\"Modified mlp_head.\")\n",
        "else:\n",
        "    print(\"Error finding head. Using fallback.\")\n",
        "    ssast_model.head = nn.Linear(768, 10)\n",
        "\n",
        "ssast_model = ssast_model.to(device)\n",
        "\n",
        "# 4. Training Loop (with Gradient Accumulation)\n",
        "optimizer = optim.Adam(ssast_model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler() # Mixed Precision for memory saving\n",
        "\n",
        "print(\"Starting Fine-Tuning (Memory Optimized)...\")\n",
        "\n",
        "for epoch in range(15):\n",
        "    ssast_model.train()\n",
        "    loss_sum = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Use Mixed Precision (FP16) to save memory\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = ssast_model(x, task='ft_avgtok')\n",
        "            loss = criterion(out, y)\n",
        "            loss = loss / ACCUM_STEPS # Scale loss\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Step optimizer every ACCUM_STEPS\n",
        "        if (i + 1) % ACCUM_STEPS == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        loss_sum += loss.item() * ACCUM_STEPS\n",
        "        _, preds = out.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += preds.eq(y).sum().item()\n",
        "\n",
        "    # Validation\n",
        "    ssast_model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                out = ssast_model(x, task='ft_avgtok')\n",
        "            _, preds = out.max(1)\n",
        "            val_total += y.size(0)\n",
        "            val_correct += preds.eq(y).sum().item()\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Epoch {epoch+1} | Loss: {loss_sum/len(train_loader):.4f} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}%\")"
      ],
      "metadata": {
        "id": "8aWKBssWGOni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e137bf0-d33b-489d-f6ce-0fa7f5e5a9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Using device: cuda\n",
            "ðŸ”„ Initializing SSAST...\n",
            "now load a SSL pretrained models from /content/drive/MyDrive/SSAST_Project/SSAST-Base-Patch-400.pth\n",
            "pretraining patch split stride: frequency=16, time=16\n",
            "pretraining patch shape: frequency=16, time=16\n",
            "pretraining patch array dimension: frequency=8, time=64\n",
            "pretraining number of patches=512\n",
            "fine-tuning patch split stride: frequncey=10, time=10\n",
            "fine-tuning number of patches=1212\n",
            "âœ… Modified mlp_head.\n",
            "ðŸš€ Starting Fine-Tuning (Memory Optimized)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3535160719.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() # Mixed Precision for memory saving\n",
            "/tmp/ipython-input-3535160719.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-3535160719.py:89: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 1.7536 | Train: 39.4% | Val: 62.4%\n",
            "Epoch 2 | Loss: 0.9016 | Train: 71.5% | Val: 67.8%\n",
            "Epoch 3 | Loss: 0.5470 | Train: 82.5% | Val: 76.6%\n",
            "Epoch 4 | Loss: 0.3384 | Train: 89.7% | Val: 82.0%\n",
            "Epoch 5 | Loss: 0.2014 | Train: 94.7% | Val: 80.0%\n",
            "Epoch 6 | Loss: 0.1397 | Train: 95.9% | Val: 81.4%\n",
            "Epoch 7 | Loss: 0.0683 | Train: 98.5% | Val: 81.4%\n",
            "Epoch 8 | Loss: 0.0607 | Train: 98.8% | Val: 80.3%\n",
            "Epoch 9 | Loss: 0.0402 | Train: 99.2% | Val: 84.1%\n",
            "Epoch 10 | Loss: 0.0214 | Train: 99.5% | Val: 84.1%\n",
            "Epoch 11 | Loss: 0.0148 | Train: 99.7% | Val: 82.4%\n",
            "Epoch 12 | Loss: 0.0103 | Train: 99.8% | Val: 82.4%\n",
            "Epoch 13 | Loss: 0.0088 | Train: 99.7% | Val: 83.1%\n",
            "Epoch 14 | Loss: 0.0105 | Train: 99.7% | Val: 84.1%\n",
            "Epoch 15 | Loss: 0.0180 | Train: 99.4% | Val: 84.4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Save Model\n",
        "SAVE_PATH = \"/content/drive/MyDrive/SSAST_Project/ssast_gtzan_finetuned.pth\"\n",
        "\n",
        "torch.save(ssast_model.state_dict(), SAVE_PATH)\n",
        "print(f\"âœ… Model saved successfully to: {SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "wXEdavSsTcRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Final Test Evaluation\n",
        "# Load Test Data (using the same Batch Size as training to be safe)\n",
        "test_ds = GTZAN_Metadata_Dataset(\"metadata/test.csv\")\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Evaluation on {len(test_ds)} Test Samples...\")\n",
        "\n",
        "ssast_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Use Mixed Precision to prevent OOM\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            out = ssast_model(x, task='ft_avgtok')\n",
        "\n",
        "        _, preds = out.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += preds.eq(y).sum().item()\n",
        "\n",
        "        # Collect data for Confusion Matrix\n",
        "        y_true.extend(y.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "final_acc = 100 * correct / total\n",
        "print(\"-\" * 30)\n",
        "print(f\"FINAL TEST ACCURACY: {final_acc:.2f}%\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "wnfwcd4lcyvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Visualizing Results\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Get class names from the dataset\n",
        "class_names = test_ds.classes\n",
        "\n",
        "# 1. Compute Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# 2. Plot Heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.title(f'Confusion Matrix (Acc: {final_acc:.1f}%)')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# 3. Print Detailed Report (Precision, Recall, F1-Score)\n",
        "print(\"\\n Detailed Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))"
      ],
      "metadata": {
        "id": "G_VHWcdSc5hU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Wh2-k5pPdIi_",
        "Qf0k-C1ofyEu",
        "elPV__umgg4O",
        "XKpXoe2WGOad",
        "vteisd-H5SeX",
        "-dHtXftddBXy"
      ],
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "16RfepYFCzLCelf0NrRTbkKmAGpQdQ4QI",
      "authorship_tag": "ABX9TyO69HXqwVNnrYs4+XZ7DOZC"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}